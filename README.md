# Дистилляция знаний с использованием представления выборки в общем латентном пространстве моделей

В работе исследуется задача дистилляции моделей глубокого обучения. Дистилляция знаний — это задача оптимизации метапараметров, в которой происходит перенос информации модели более сложной структуры, называемой моделью-учителем, в модель более простой структуры, называемой моделью-учеником. В работе рассматривается случай переноса разнородных данных от нескольких моделей-учителей к модели-ученику. В данной работе предлагается построить аналог уже существующих решений с использованием функции оптимизации с триплетными ограничениями, которая показывала более хорошие результаты для классической дистилляции. Вычислительный эксперимент производится на выборке CIFAR-10.
