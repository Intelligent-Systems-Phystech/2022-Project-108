\documentclass{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}




\title{Дистилляция знаний с использованием представления выборки в общем латентном пространстве моделей}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{Седова Анна \\
	\texttt{sedova.aa@phystech.edu} \\
    \And
	Горпинич Мария\\
	\texttt{gorpinich.m@phystech.edu} \\
	\And
	Бахтеев Олег \\
	\texttt{bakhteev@phystech.edu} \\
	\And
	Стрижов Вадим \\
	\texttt{} \\
}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Дистилляция знаний}
}

\begin{document}
\maketitle

\begin{abstract}
В работе исследуется задача дистилляции моделей глубокого обучения. Дистилляция знаний — это задача оптимизации метапараметров, в которой происходит перенос информации модели более сложной структуры, называемой моделью-учителем, в модель более простой структуры, называемой моделью-учеником. В работе рассматривается случай переноса разнородных данных от нескольких моделей-учителей к модели-ученику. В данной работе предлагается построить аналог уже существующих решений с использованием функции оптимизации с триплетными ограничениями, которая показывала более хорошие результаты для классической дистилляции. Вычислительный эксперимент производится на выборке CIFAR-10.
\end{abstract}



\section{Введение}

В работе рассматривается разнородная дистилляция знаний с  несколькими учителями с использованием промежуточных скрытых представлений учителей и ученика. Оптимизация модели глубокого обучения является вычислительно сложной задачей. Дистилляция знаний является частным случаем оптимизации моделей глубокого обучения.

Дистилляцией знаний называется способ оптимизации \textit{модели-ученика}, при котором учитывается не только информация, содержащаяся в выборке, но и информация, содержащаяся в \textit{модели-учителе}, имеющей высокую сложность. Разнородной дистилляцией знаний с несколькими учителями называется обобщение классической дистилляции знаний, в котором есть несколько моделей-учителей, каждый из которых обладает своей собственной, неполной информацией о выборке.

Для выравнивания скрытых пространств предлагается использовать функцию оптимизации с триплетными ограничениями. \textit{Функцией оптимизации с триплетными ограничениями} называется функция, имеющая следующий вид $E =
\sum_{(a,p,n)∈Θ}
max(0, m + ||f(x_a)  −  f(x_p)||_2^2 
- ||f(x_a)  −  f(x_n)||_2^2
)$.
При решении задачи оптимизации с данной функцией потерь расстояние между $x_a$, называемым \textit{якорем}, и $x_p$, называемым \textit{положительным входом}, минимизируется, а расстояние между якорем и $x_n$, называемым \textit{отрицательным входом}, максимизируется.

Сложность дистилляции знаний с помощью скрытого представления о выборке моделей состоит в том, что информация из промежуточных представлений разных моделей может иметь разную размерность. В данной работе предлагается способ сведения этой информации в одно латентное пространство.

Классический случай дистиляяции с одним учителем рассматривается в работе \citep{hinton}. В работах \citep{tripletloss,tripletloss-face-rec-distill} для обучения модели-ученика используется функция оптимизации с триплетными ограничениями. Данное исследование  показывает, что эта функция дает более точные результаты на CIFAR 10 и Tiny Image Net. Это дает основания полагать, что и для разнородной дистилляции с несколькими учителями функция оптимизации с триплетными ограничениями может давать более хорошие результаты. 

Вариации дистилляции с несколькими учителями рассматриваются в \citep{multi-teacher,multi-teacher-multi-level, adaptive}.  

 Предлагается рассмотреть скрытые представления учителей и ученика получаемые при помощи алгоритмов снижения размерности. Для выравнивания пространств моделей предлагается применять модель автокодировщика с триплетными ограничениями \citep{tripletloss}. Предложенноые ранее решения \cite{multi-teacher}, {multi-teacher-multi-level} не использует функцию оптимизации с триплетными ограничениями. \citep{multi-teacher} также не учитывает, что учителя могут иметь разную информацию о выборке. \citep{tripletloss-face-rec-distill} использует функцию оптимизации с триплетными ограничениями, но
не выравнивает информацию в одно латентное пространство. 

Вычислительный эксперимент производится на выборке CIFAR-10. (Тут нужно что-то дописать)





\label{sec:others}
\bibliographystyle{plainurl}
\nocite{*}
\bibliography{references}  

\end{document}