@article{hinton,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Oriol Vinyals, Jeff Dean},
  year={2015},
  organization={Google Inc.}
}

@article{tripletloss,
  title={Triplet Loss for Knowledge Distillation},
  author={Hideki Oki, Motoshi Abe, Jyunichi Miyao and Takio Kurita},
  year={2020},
  organization={Hiroshima University}
}

@article{multi-teacher-multi-level,
  title={Adaptive Multi-Teacher Multi-level Knowledge Distillation},
  author={Yuang Liu, Wei Zhang, Jun Wang},
  year={2021},
  organization={East China Normal University}
}

@article{multi-teacher,
  title={Multiple Teacher Distillation for Robust and Greener Models},
  author={Artur Ilichev, Nikita Sorokin, Valentin Malykh, Irina Piontkovskaya},
  year={2021},
  organization={Huawei Noah’s Ark lab, Moscow Institute
of Physics and Technology}
}

@article{tripletloss-face-rec-distill,
  title={Teacher–student training and triplet loss to reduce the effect of drastic
face occlusion},
  author={Mariana-Iuliana Georgescu, Georgian-Emilian Du¸t ˇa,  Radu Tudor Ionescu},
  year={2015},
}

@article{adaptive,
  title={Adaptive Distillation: Aggregating Knowledge from Multiple Paths for Efficient
Distillation},
  author={Sumanth Chennupati, Mohammad Mahdi Kamani, Zhongwei Cheng, Lin Chen},
  year={2021},
  organization={WYZE Labs AI Team, Kirkland, WA, USA}
}

@article{tripletloss-face-rec,
  title={FaceNet: A Unified Embedding for Face Recognition and Clustering},
  author={Florian Schroff, Dmitry Kalenichenko, James Philbin},
  year={2015},
  organization={Google Inc}
}